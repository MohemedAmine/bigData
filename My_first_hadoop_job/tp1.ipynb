{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_params(nx, nh, ny):\n",
    "    \"\"\"\n",
    "    Initializes the weights for an MLP with:\n",
    "    - nx: number of input neurons\n",
    "    - nh: number of hidden neurons\n",
    "    - ny: number of output neurons\n",
    "    \n",
    "    All weights are initialized following a normal distribution with mean=0 and std=0.3.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing the initialized weights and biases:\n",
    "        - W1: weights for the input to hidden layer\n",
    "        - b1: biases for the hidden layer\n",
    "        - W2: weights for the hidden to output layer\n",
    "        - b2: biases for the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    W1 = np.random.normal(0, 0.3, (nh, nx))  # Weights from input to hidden layer\n",
    "    b1 = np.zeros((nh, 1))  # Biases for hidden layer\n",
    "    \n",
    "    W2 = np.random.normal(0, 0.3, (ny, nh))  # Weights from hidden to output layer\n",
    "    b2 = np.zeros((ny, 1))  # Biases for output layer\n",
    "    \n",
    "    # Store the parameters in a dictionary\n",
    "    params = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward(params, X):\n",
    "    \"\"\"\n",
    "    Perform a forward pass through the MLP.\n",
    "\n",
    "    Arguments:\n",
    "    - params: dictionary containing the parameters ('W1', 'b1', 'W2', 'b2')\n",
    "    - X: input data of shape (n_batch, nx), where n_batch is the batch size and nx is the number of input neurons.\n",
    "\n",
    "    Returns:\n",
    "    - Y_hat: output predictions (after softmax), of shape (n_batch, ny)\n",
    "    - cache: dictionary containing intermediate values for backpropagation (Z1, A1, Z2, A2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from the dictionary\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    \n",
    "    # Forward pass for the hidden layer\n",
    "    Z1 = np.dot(X, W1.T) + b1.T  # Z1 = X * W1^T + b1\n",
    "    A1 = np.tanh(Z1)  # A1 = tanh(Z1) for the hidden layer activation\n",
    "    \n",
    "    # Forward pass for the output layer\n",
    "    Z2 = np.dot(A1, W2.T) + b2.T  # Z2 = A1 * W2^T + b2\n",
    "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=1, keepdims=True)  # A2 = softmax(Z2)\n",
    "    \n",
    "    # Store intermediate values in cache for backpropagation\n",
    "    cache = {\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2\n",
    "    }\n",
    "    \n",
    "    # The output of the network is A2 (the softmax output)\n",
    "    Y_hat = A2\n",
    "    \n",
    "    return Y_hat, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_accuracy(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Compute the loss (cross-entropy) and accuracy of the model.\n",
    "\n",
    "    Arguments:\n",
    "    - Y_hat: predicted values (after softmax), of shape (n_batch, ny)\n",
    "    - Y: true labels, of shape (n_batch, ny), one-hot encoded for multi-class classification.\n",
    "\n",
    "    Returns:\n",
    "    - loss: the computed cross-entropy loss.\n",
    "    - accuracy: the computed accuracy as a percentage of correct predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of examples in the batch\n",
    "    n_batch = Y.shape[0]\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    # We use np.log to compute the log of predicted probabilities\n",
    "    loss = -np.sum(Y * np.log(Y_hat)) / n_batch\n",
    "    \n",
    "    # Accuracy calculation\n",
    "    # We compare the predicted class (the class with the highest probability) with the true class\n",
    "    predictions = np.argmax(Y_hat, axis=1)  # Get the index of the class with the highest probability\n",
    "    true_labels = np.argmax(Y, axis=1)  # Get the index of the true class\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = np.sum(predictions == true_labels)\n",
    "    \n",
    "    # Accuracy as the percentage of correct predictions\n",
    "    accuracy = correct_predictions / n_batch * 100\n",
    "    \n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
